{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FaKMuBSZ9-aD"
   },
   "source": [
    "# Train a U-Net Model from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIllmxrJHWvA"
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n",
      "True\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import cv2\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import Model, layers, models\n",
    "\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from tensorflow.keras.preprocessing.image import Iterator, ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.preprocessing.image import Iterator, ImageDataGenerator\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tf.test.is_built_with_cuda()) \n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "import skimage.transform\n",
    "\n",
    "import napari\n",
    "\n",
    "# tf.config.gpu.set_per_process_memory_fraction(0.75)\n",
    "# tf.config.gpu.set_per_process_memory_growth(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageMaskGenerator(Sequence):\n",
    "    \"\"\"\n",
    "    Generates images and masks for performing data augmentation in Keras.\n",
    "    We inherit from Sequence (instead of directly using the keras ImageDataGenerator)\n",
    "    since we want to perform augmentation on both the input image AND the mask \n",
    "    (target). This mechanism needs to be implemented in this class. This class\n",
    "    also allows to implement new augmentation transforms that are not implemented\n",
    "    in the core Keras class (illumination, etc.).\n",
    "    See : https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "    and https://stackoverflow.com/questions/56758592/how-to-customize-imagedatagenerator-in-order-to-modify-the-target-variable-value\n",
    "    for more details.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X_set, Y_set=None, # input images and masks\n",
    "                 batch_size: int=32, dim: tuple=(512, 512),\n",
    "                 n_channels_ims: int=1, n_channels_masks: int=1, # informations \n",
    "                 shuffle: bool=True, normalize=True, reshape=False, crop=None,# preprocessing params\n",
    "                 **kwargs): # data augmentation params\n",
    "        \"\"\"\n",
    "        X_set (list, array or str): pointer to the images (Bright-Field). If str\n",
    "        the string is assumed to be pointing at some directory.\n",
    "        Y_set (list; array or str): pointer to the masks (target). If str\n",
    "        the string is assumed to be pointing at some directory.\n",
    "        batch_size (int): size of the batch\n",
    "        dim (tuple): dimension of the images\n",
    "        n_channels_ims (int) : number of channels of the images (1 for TIF)\n",
    "        shuffle (bool): Shuffle the dataset between each training epoch\n",
    "        crop (tuple): Target dim of one image after cropping\n",
    "        normalize (bool): normalize the images and masks in the beginning\n",
    "        reshape (bool): reshape the images and masks to (dim, dim, n_channels_ims)\n",
    "        histogram_equalization (bool): perform histogram equalization to improve\n",
    "        rendering using opencv\n",
    "        horiz_flip_percent ()\n",
    "        vert_flip_percent\n",
    "        \"\"\"\n",
    "        # super().__init__(n, batch_size, shuffle, seed)\n",
    "        self.dim = dim\n",
    "        self.im_size = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.n_channels_ims = n_channels_ims\n",
    "        self.n_channels_masks = n_channels_masks\n",
    "        \n",
    "        # build the X_set in an array. If X_set is a directory containing images\n",
    "        # then self.X_set doesn't contains the images but the file names, but it\n",
    "        # is transparent for the user.\n",
    "        if type(X_set) == list:\n",
    "            self.from_directory_X = False\n",
    "            self.X_set = np.array(X_set)\n",
    "        elif type(X_set) == np.array:\n",
    "            self.from_directory_X = False\n",
    "            self.X_set = X_set\n",
    "        elif type(X_set) == str: # assuming a path\n",
    "            self.from_directory_X = True\n",
    "            self.X_dir = X_set # path to the images dir\n",
    "#             if self.n_channels_ims == 1:\n",
    "#                 self.X_set = np.array(sorted(os.listdir(X_set))) # sorted guarantees the order\n",
    "#             else: # n_channels_ims > 1 : several channels per image\n",
    "            self.X_set = []\n",
    "            for k in range(0, len(os.listdir(X_set)), self.n_channels_ims):\n",
    "                self.X_set.append(np.array(os.listdir(X_set)[k:k+self.n_channels_ims]))\n",
    "            self.X_set = np.array(self.X_set)\n",
    "        else:\n",
    "            raise TypeError(\"X_set should be list, array or path\")\n",
    "        \n",
    "        # build the Y_set in an array\n",
    "        if type(Y_set) == list:\n",
    "            self.from_directory_Y = False\n",
    "            self.Y_set = np.array(Y_set)\n",
    "        elif type(Y_set) == np.array:\n",
    "            self.from_directory_Y = False\n",
    "            self.Y_set = Y_set\n",
    "        elif type(Y_set) == str: # assuming a path\n",
    "            self.from_directory_Y = True\n",
    "            self.Y_dir = Y_set # path to the masks dir\n",
    "            self.Y_set = []\n",
    "            for k in range(0, len(os.listdir(Y_set)), self.n_channels_masks):\n",
    "                self.Y_set.append(np.array(os.listdir(Y_set)[k:k+self.n_channels_masks]))\n",
    "            self.Y_set = np.array(self.Y_set)\n",
    "        else:\n",
    "            raise TypeError(\"Y_set should be list, array or path\")\n",
    "\n",
    "        # Check if there are the same number of images in X (images) and Y (masks)\n",
    "        assert self.X_set.shape[0] != 0 and self.Y_set.shape[0] != 0, print(f\"Directory '{X_set}' is empty!\")\n",
    "        assert self.X_set.shape[0] == self.Y_set.shape[0], print(f\"{self.X_set.shape[0]} images != {self.Y_set.shape[0]} masks\")\n",
    "\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "        # Preprocessing parameters\n",
    "        self.normalize = normalize\n",
    "        self.reshape = reshape\n",
    "        self.crop = crop\n",
    "\n",
    "        # The Keras generator that will be used to perform data augmentation \n",
    "        self.generator = ImageDataGenerator(**kwargs)\n",
    "\n",
    "        # Initialize the indices (shuffle if asked)\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Number of batches per epoch : we evenly split the train set into samples\n",
    "        of size batch_size.\n",
    "        \"\"\"\n",
    "        return int(np.floor(self.X_set.shape[0] / self.batch_size))\n",
    "        \n",
    "    def __getitem__(self, index: int):\n",
    "        \"\"\"\n",
    "        Generate one batch of data.\n",
    "        \"\"\"\n",
    "        if index >= self.__len__():\n",
    "            raise IndexError\n",
    "        \n",
    "        # Generate indices corresponding to the images in the batch\n",
    "        indices = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Generate the batch\n",
    "        X, Y = self.__data_generation(indices)\n",
    "        return X, Y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"\n",
    "        Updates indexes after each epoch. self.indexes is used to retrieve the\n",
    "        samples and organize them into batches.\n",
    "        If shuffle : randomizes the order of the samples in order to give \n",
    "        different training batches at each epoch.\n",
    "        \"\"\"\n",
    "        self.indexes = np.arange(self.X_set.shape[0])\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs: [int]):\n",
    "        \"\"\"\n",
    "        Generates data containing batch_size samples. This is where we load the\n",
    "        images if they are in a directory, and apply transformations to them.\n",
    "        \"\"\" \n",
    "        # Load data (from directory or from X_set depending on the given data)\n",
    "        if self.from_directory_X:\n",
    "            batch_X = []\n",
    "            for im in list_IDs:\n",
    "                channels = []\n",
    "                for k in range(self.n_channels_ims):\n",
    "                    channels.append(np.expand_dims(imageio.imread(f\"{self.X_dir}/{self.X_set[im, k]}\"), axis=-1)) # add channel axis\n",
    "                batch_X.append(np.concatenate(channels, axis=-1))\n",
    "            batch_X = np.array(batch_X)\n",
    "        else:\n",
    "            batch_X = self.X_set[list_IDs]\n",
    "\n",
    "        if self.from_directory_Y:\n",
    "            batch_Y = []\n",
    "            for im in list_IDs:\n",
    "                channels = []\n",
    "                for k in range(self.n_channels_masks):\n",
    "                    channels.append(np.expand_dims(imageio.imread(f\"{self.Y_dir}/{self.Y_set[im, k]}\"), axis=-1)) # add channel axis\n",
    "                batch_Y.append(np.concatenate(channels, axis=-1))\n",
    "            batch_Y = np.array(batch_Y) \n",
    "        else:\n",
    "            batch_Y = self.Y_set[list_IDs]\n",
    "\n",
    "        # Preprocessing\n",
    "        if self.crop is not None:\n",
    "            batch_X = self.perf_crop(batch_X)\n",
    "            batch_Y = self.perf_crop(batch_Y)\n",
    "\n",
    "        if self.reshape:\n",
    "            batch_X = self.perf_reshape(batch_X, is_images=True)\n",
    "            batch_Y = self.perf_reshape(batch_Y, is_images=False)\n",
    "\n",
    "        if self.normalize:\n",
    "            batch_X = self.perf_normalize(batch_X)\n",
    "            batch_Y = self.perf_normalize(batch_Y)\n",
    "\n",
    "#         if self.n_channels_ims == 3:\n",
    "#             batch_X = np.concatenate([batch_X, batch_X, batch_X], axis=-1)\n",
    "\n",
    "        # Perform the SAME transformation on the image and on the mask\n",
    "        for i, (img, mask) in enumerate(zip(batch_X, batch_Y)):\n",
    "            transform_params = self.generator.get_random_transform(img.shape)\n",
    "            batch_X[i] = self.generator.apply_transform(img, transform_params)\n",
    "            batch_Y[i] = self.generator.apply_transform(mask, transform_params)\n",
    "\n",
    "        return batch_X, batch_Y        \n",
    "\n",
    "    # Preprocessing functions\n",
    "    def perf_crop(self, images):\n",
    "        crop_X = int((images.shape[1] - self.crop[0]) // 2)\n",
    "        crop_Y = int((images.shape[2] - self.crop[1]) // 2)\n",
    "        assert (crop_X >= 0 and crop_Y >= 0), print(f\"Target size after cropping {self.crop} should be lower than the initial shape {(images.shape[1], images.shape[2])}.\")\n",
    "        new_batch = np.empty((self.batch_size, *self.crop, images.shape[3]))\n",
    "        for i, img in enumerate(images):\n",
    "            if crop_X != 0 and crop_Y != 0:\n",
    "                new_batch[i] = img[crop_X:-crop_X, crop_Y:-crop_Y]\n",
    "            elif crop_X != 0:\n",
    "                new_batch[i] = img[crop_X:-crop_X, :]\n",
    "            elif crop_Y != 0:\n",
    "                new_batch[i] = img[:, crop_Y:-crop_Y]\n",
    "            else:\n",
    "                new_batch[i] = img\n",
    "        return new_batch\n",
    "\n",
    "    def perf_reshape(self, images, is_images=True):\n",
    "        \"\"\"\n",
    "        images (np.array): batch of images of shape (batch_size, n_rows, n_cols, n_chans)\n",
    "        is_images (bool): is it a batch of images (True) or masks (False)\n",
    "        \"\"\"\n",
    "        if is_images:  # batch of images\n",
    "            new_batch = np.empty((self.batch_size, *self.im_size, self.n_channels_ims))\n",
    "            for i, img in enumerate(images): # the resize function normalizes the images anyways...\n",
    "                new_batch[i] = skimage.transform.resize(img, (*self.im_size, self.n_channels_ims), anti_aliasing=True)\n",
    "        else:  # batch of masks\n",
    "            new_batch = np.empty((self.batch_size, *self.im_size, self.n_channels_masks))\n",
    "            for i, img in enumerate(images):\n",
    "                new_batch[i] = skimage.transform.resize(img, (*self.im_size, self.n_channels_masks), anti_aliasing=True)\n",
    "        return new_batch\n",
    "\n",
    "    def perf_normalize(self, images):\n",
    "        \"\"\"\n",
    "        Performs per image, per channel normalization by substracting the min and dividing by (max - min)\n",
    "        \"\"\"\n",
    "        new_batch = np.empty(images.shape)\n",
    "        for i, img in enumerate(images):\n",
    "            assert (np.min(img, axis=(0, 1)) != np.max(img, axis=(0, 1))).all(), print(\"Cannot normalize an image containing only 0 or 1 valued pixels. There is likely an empty image in the training set.\\nIf cropping was used,\"\n",
    "                                                                                       \"maybe the mask doesn't contain any white pixel in the specific region.\")\n",
    "            new_batch[i] = (img - np.min(img, axis=(0, 1))) / (np.max(img, axis=(0, 1)) - np.min(img, axis=(0, 1)))\n",
    "        return new_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"D:/Hugo/BiSeg/Train_Set\"\n",
    "bf_dir, mask_dir = f\"{data_path}/images/\", f\"{data_path}/masks/\"\n",
    "\n",
    "# cf. la doc Keras pour voir tout ce qu'il est possible de faire\n",
    "# https://keras.io/api/preprocessing/image/\n",
    "# voir aussi la librairie imgaug ou albumentation pour implementer de nouvelles transfo\n",
    "augmentation_params = dict(zoom_range=[0.9, 1.5],\n",
    "                           rotation_range=360,\n",
    "                           height_shift_range=0.2,\n",
    "                           width_shift_range=0.2,\n",
    "                           fill_mode=\"constant\", cval=0)\n",
    "\n",
    "# augmentation_params = {}\n",
    "\n",
    "bat_size, nc_ims, nc_masks, shuffle = 4, 2, 1, True  # SPECIFY HERE THE NUMBER OF CHANNELS\n",
    "crop, reshape, target_dim, normalize = None, True, (512, 512), True\n",
    "\n",
    "generator = ImageMaskGenerator(bf_dir, mask_dir, \n",
    "                               batch_size=bat_size, dim=target_dim, n_channels_ims=nc_ims, n_channels_masks=nc_masks, \n",
    "                               shuffle=shuffle, normalize=normalize, reshape=reshape, crop=crop,\n",
    "                                **augmentation_params)\n",
    "\n",
    "val_generator = ImageMaskGenerator(f\"{data_path}/val_images/\", f\"{data_path}/val_masks/\", \n",
    "                               batch_size=1, dim=target_dim, n_channels_ims=nc_ims, n_channels_masks=nc_masks,\n",
    "                               shuffle=shuffle, normalize=normalize, reshape=reshape, crop=crop,\n",
    "                                **augmentation_params)\n",
    "\n",
    "def visualize_data(bf, masks, nc_ims):\n",
    "    with napari.gui_qt():\n",
    "        if nc_ims == 1:\n",
    "            viewer = napari.view_image(bf[:, :, :, :].squeeze(-1))\n",
    "            viewer.add_image(masks[:, :, :, :].squeeze(-1), blending=\"additive\")\n",
    "        else:\n",
    "            viewer = napari.view_image(bf[:, :, :, 1])  # bf\n",
    "            viewer.add_image(bf[:, :, :, 0], blending=\"additive\")\n",
    "            viewer.add_image(masks[:, :, :, :].squeeze(-1), blending=\"additive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Batches : 12\n"
     ]
    }
   ],
   "source": [
    "plot = True\n",
    "if plot:\n",
    "    print(f\"# Batches : {len(generator)}\")\n",
    "    bf, masks = generator[11]\n",
    "    bf, masks = np.array(bf), np.array(masks)\n",
    "    \n",
    "    visualize_data(bf, masks, nc_ims=nc_ims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7YbX33TwJ6S_"
   },
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unet(nbr, x, y, n_channels_imgs=1, n_channels_masks=1):\n",
    "    \"\"\"\n",
    "    nbr (int): kernel side\n",
    "    x (int): image height\n",
    "    y (int): image width\n",
    "    \"\"\"\n",
    "    print(f\"# input channels : {n_channels_imgs}.\")\n",
    "    print(f\"# output channels : {n_channels_masks}.\")\n",
    "    \n",
    "    initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=1.)\n",
    "    entree=layers.Input(shape=(x, y, n_channels_imgs), dtype='float16')\n",
    "\n",
    "    result=layers.Conv2D(nbr, 3, activation='relu', padding='same', kernel_initializer=initializer)(entree)\n",
    "    result=layers.BatchNormalization()(result)\n",
    "#     result=layers.Dropout(0.2)\n",
    "    result=layers.Conv2D(nbr, 3, activation='relu', padding='same', kernel_initializer=initializer)(result)\n",
    "    result1=layers.BatchNormalization()(result)\n",
    "\n",
    "    result=layers.MaxPool2D()(result1)\n",
    "\n",
    "    result=layers.Conv2D(2*nbr, 3, activation='relu', padding='same', kernel_initializer=initializer)(result)\n",
    "    result=layers.BatchNormalization()(result)\n",
    "#     result=layers.Dropout(0.2)\n",
    "    result=layers.Conv2D(2*nbr, 3, activation='relu', padding='same', kernel_initializer=initializer)(result)\n",
    "    result2=layers.BatchNormalization()(result)\n",
    "\n",
    "    result=layers.MaxPool2D()(result2)\n",
    "\n",
    "    result=layers.Conv2D(4*nbr, 3, activation='relu', padding='same', kernel_initializer=initializer)(result)\n",
    "    result=layers.BatchNormalization()(result)\n",
    "#     result=layers.Dropout(0.2)\n",
    "    result=layers.Conv2D(4*nbr, 3, activation='relu', padding='same', kernel_initializer=initializer)(result)\n",
    "    result3=layers.BatchNormalization()(result)\n",
    "\n",
    "    result=layers.MaxPool2D()(result3)\n",
    "\n",
    "    result=layers.Conv2D(4*nbr, 3, activation='relu', padding='same', kernel_initializer=initializer)(result)\n",
    "    result=layers.BatchNormalization()(result)\n",
    "#     result=layers.Dropout(0.2)\n",
    "    result=layers.Conv2D(4*nbr, 3, activation='relu', padding='same', kernel_initializer=initializer)(result)\n",
    "    result4=layers.BatchNormalization()(result)\n",
    "\n",
    "    result=layers.MaxPool2D()(result4)\n",
    "\n",
    "    result=layers.Conv2D(8*nbr, 3, activation='relu', padding='same', kernel_initializer=initializer)(result)\n",
    "    result=layers.BatchNormalization()(result)\n",
    "#     result=layers.Dropout(0.2)\n",
    "    result=layers.Conv2D(4*nbr, 3, activation='relu', padding='same', kernel_initializer=initializer)(result)\n",
    "    result=layers.BatchNormalization()(result)\n",
    "\n",
    "    result=layers.UpSampling2D()(result)\n",
    "    result=tf.concat([result, result4], axis=3)\n",
    "\n",
    "    result=layers.Conv2D(8*nbr, 3, activation='relu', padding='same', kernel_initializer=initializer)(result)\n",
    "    result=layers.BatchNormalization()(result)\n",
    "#     result=layers.Dropout(0.2)\n",
    "    result=layers.Conv2D(4*nbr, 3, activation='relu', padding='same', kernel_initializer=initializer)(result)\n",
    "    result=layers.BatchNormalization()(result)\n",
    "\n",
    "    result=layers.UpSampling2D()(result)\n",
    "    result=tf.concat([result, result3], axis=3)\n",
    "\n",
    "    result=layers.Conv2D(4*nbr, 3, activation='relu', padding='same', kernel_initializer=initializer)(result)\n",
    "    result=layers.BatchNormalization()(result)\n",
    "#     result=layers.Dropout(0.2)\n",
    "    result=layers.Conv2D(2*nbr, 3, activation='relu', padding='same', kernel_initializer=initializer)(result)\n",
    "    result=layers.BatchNormalization()(result)\n",
    "\n",
    "    result=layers.UpSampling2D()(result)\n",
    "    result=tf.concat([result, result2], axis=3)\n",
    "\n",
    "    result=layers.Conv2D(2*nbr, 3, activation='relu', padding='same', kernel_initializer=initializer)(result)\n",
    "    result=layers.BatchNormalization()(result)\n",
    "#     result=layers.Dropout(0.2)\n",
    "    result=layers.Conv2D(nbr, 3, activation='relu', padding='same', kernel_initializer=initializer)(result)\n",
    "    result=layers.BatchNormalization()(result)\n",
    "\n",
    "    result=layers.UpSampling2D()(result)\n",
    "    result=tf.concat([result, result1], axis=3)\n",
    "\n",
    "    result=layers.Conv2D(nbr, 3, activation='relu', padding='same', kernel_initializer=initializer)(result)\n",
    "    result=layers.BatchNormalization()(result)\n",
    "#     result=layers.Dropout(0.2)\n",
    "    result=layers.Conv2D(nbr, 3, activation='relu', padding='same', kernel_initializer=initializer)(result)\n",
    "    result=layers.BatchNormalization()(result)\n",
    "\n",
    "    sortie=layers.Conv2D(n_channels_masks, 1, activation='sigmoid', padding='same', kernel_initializer=initializer)(result)\n",
    "\n",
    "    model=models.Model(inputs=entree, outputs=sortie)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DtOkjG_hrRsl"
   },
   "source": [
    "## Loss function : Weighted binary crossentropy\r\n",
    "\r\n",
    "This step is important because we are facing a class imbalance problem : the 0 class (i.e. background) are way more numerous than the 1 class (i.e. yeast pixels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not necessary for whole cells\n",
    "# TODO: try it on mating or buds\n",
    "\n",
    "class WeightedBinaryCrossEntropy():\n",
    "\n",
    "    def __init__(self, class_weight={0: 0.5, 1: 0.5}):\n",
    "        self.class_weight = class_weight\n",
    "        self.__name__ = \"binary_cross_entropy\"\n",
    "\n",
    "    def __call__(self, Y_true, Y_pred):\n",
    "        \"\"\"\n",
    "        Compute the weights binary cross entropy for a given mask Y_true and a given\n",
    "        prediction Y_pred.\n",
    "        \"\"\"\n",
    "        sample_weight = {0: 0.2, 1: 0.8}\n",
    "        y_true = K.clip(Y_true, K.epsilon(), 1-K.epsilon())\n",
    "        y_pred = K.clip(Y_pred, K.epsilon(), 1-K.epsilon())\n",
    "        logloss = -(y_true * K.log(y_pred) * self.class_weight[1] \n",
    "                    + (1 - y_true) * K.log(1 - y_pred) * self.class_weight[0] )\n",
    "        return K.mean(logloss, axis=-1)\n",
    "\n",
    "weights = {0: 1, 1: 100}\n",
    "binary_cross_entropy = WeightedBinaryCrossEntropy(class_weight=weights)\n",
    "\n",
    "def jaccard_distance(smooth=20):\n",
    "\n",
    "    def jaccard_distance_fixed(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Calculates mean of Jaccard distance as a loss function\n",
    "        \"\"\"\n",
    "        intersection = tf.reduce_sum(y_true * y_pred, axis=(1,2))\n",
    "        sum_ = tf.reduce_sum(y_true + y_pred, axis=(1,2))\n",
    "        jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "        jd =  (1 - jac) * smooth\n",
    "        return tf.reduce_mean(jd)\n",
    "    \n",
    "    return jaccard_distance_fixed\n",
    "\n",
    "def binary_focal_loss(gamma=2., alpha=.25):\n",
    "    \"\"\"\n",
    "    Binary form of focal loss.\n",
    "    FL(p_t) = -alpha * (1 - p_t)**gamma * log(p_t)\n",
    "    where p = sigmoid(x), p_t = p or 1 - p depending on if the label is 1 or 0, respectively.\n",
    "    References:\n",
    "        https://arxiv.org/pdf/1708.02002.pdf\n",
    "    Usage:\n",
    "    model.compile(loss=[binary_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
    "    \"\"\"\n",
    "\n",
    "    def binary_focal_loss_fixed(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        :param y_true: A tensor of the same shape as `y_pred`\n",
    "        :param y_pred:  A tensor resulting from a sigmoid\n",
    "        :return: Output tensor.\n",
    "        \"\"\"\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        # Define epsilon so that the back-propagation will not result in NaN for 0 divisor case\n",
    "        epsilon = K.epsilon()\n",
    "        # Add the epsilon to prediction value\n",
    "        # y_pred = y_pred + epsilon\n",
    "        # Clip the prediciton value\n",
    "        y_pred = K.clip(y_pred, epsilon, 1.0 - epsilon)\n",
    "        # Calculate p_t\n",
    "        p_t = tf.where(K.equal(y_true, 1), y_pred, 1 - y_pred)\n",
    "        # Calculate alpha_t\n",
    "        alpha_factor = K.ones_like(y_true) * alpha\n",
    "        alpha_t = tf.where(K.equal(y_true, 1), alpha_factor, 1 - alpha_factor)\n",
    "        # Calculate cross entropy\n",
    "        cross_entropy = -K.log(p_t)\n",
    "        weight = alpha_t * K.pow((1 - p_t), gamma)\n",
    "        # Calculate focal loss\n",
    "        loss = weight * cross_entropy\n",
    "        # Sum the losses in mini_batch\n",
    "        loss = K.mean(K.sum(loss, axis=1))\n",
    "        return loss\n",
    "    return binary_focal_loss_fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ik4CeRhs1v9J"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smoothing : [50]\n",
      "# input channels : 2.\n",
      "# output channels : 1.\n",
      "WARNING:tensorflow:Layer conv2d_209 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer conv2d_209 is casting an input tensor from dtype float16 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float16 by default, call `tf.keras.backend.set_floatx('float16')`. To change just this layer, pass dtype='float16' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "12/12 [==============================] - 26s 2s/step - loss: 47.7547 - mean_IoU: 0.4421 - val_loss: 47.6290 - val_mean_IoU: 0.4991 - lr: 0.0050\n",
      "Epoch 2/300\n",
      "12/12 [==============================] - 24s 2s/step - loss: 46.3714 - mean_IoU: 0.4604 - val_loss: 47.5196 - val_mean_IoU: 0.4992 - lr: 0.0050\n",
      "Epoch 3/300\n",
      "12/12 [==============================] - 24s 2s/step - loss: 44.9747 - mean_IoU: 0.4711 - val_loss: 47.6654 - val_mean_IoU: 0.4992 - lr: 0.0050\n",
      "Epoch 4/300\n",
      "12/12 [==============================] - 23s 2s/step - loss: 45.4524 - mean_IoU: 0.4762 - val_loss: 47.8992 - val_mean_IoU: 0.4990 - lr: 0.0050\n",
      "Epoch 5/300\n",
      "12/12 [==============================] - 24s 2s/step - loss: 44.7701 - mean_IoU: 0.4966 - val_loss: 48.6206 - val_mean_IoU: 0.4981 - lr: 0.0050\n",
      "Epoch 6/300\n",
      "12/12 [==============================] - 23s 2s/step - loss: 44.5427 - mean_IoU: 0.5024 - val_loss: 48.8208 - val_mean_IoU: 0.4981 - lr: 0.0050\n",
      "Epoch 7/300\n",
      "12/12 [==============================] - 24s 2s/step - loss: 43.5275 - mean_IoU: 0.5170 - val_loss: 48.8440 - val_mean_IoU: 0.4982 - lr: 0.0050\n",
      "Epoch 8/300\n",
      "12/12 [==============================] - 23s 2s/step - loss: 43.1921 - mean_IoU: 0.5310 - val_loss: 48.2532 - val_mean_IoU: 0.4990 - lr: 0.0050\n",
      "Epoch 9/300\n",
      "12/12 [==============================] - 23s 2s/step - loss: 42.6642 - mean_IoU: 0.5295 - val_loss: 48.0397 - val_mean_IoU: 0.4991 - lr: 0.0050\n",
      "Epoch 10/300\n",
      "12/12 [==============================] - 24s 2s/step - loss: 42.5482 - mean_IoU: 0.5352 - val_loss: 48.3211 - val_mean_IoU: 0.4988 - lr: 0.0050\n",
      "Epoch 11/300\n",
      "12/12 [==============================] - 24s 2s/step - loss: 42.1060 - mean_IoU: 0.5258 - val_loss: 48.6172 - val_mean_IoU: 0.5006 - lr: 0.0050\n",
      "Epoch 12/300\n",
      "12/12 [==============================] - 24s 2s/step - loss: 42.6472 - mean_IoU: 0.5264 - val_loss: 47.0338 - val_mean_IoU: 0.5033 - lr: 0.0050\n",
      "Epoch 13/300\n",
      "12/12 [==============================] - 24s 2s/step - loss: 38.0948 - mean_IoU: 0.5479 - val_loss: 48.0794 - val_mean_IoU: 0.4990 - lr: 0.0050\n",
      "Epoch 14/300\n",
      "12/12 [==============================] - 24s 2s/step - loss: 40.7703 - mean_IoU: 0.5434 - val_loss: 43.7795 - val_mean_IoU: 0.5207 - lr: 0.0050\n",
      "Epoch 15/300\n",
      "12/12 [==============================] - 23s 2s/step - loss: 38.7194 - mean_IoU: 0.5655 - val_loss: 46.0013 - val_mean_IoU: 0.5212 - lr: 0.0050\n",
      "Epoch 16/300\n",
      "12/12 [==============================] - 23s 2s/step - loss: 37.6838 - mean_IoU: 0.5601 - val_loss: 47.5574 - val_mean_IoU: 0.4992 - lr: 0.0050\n",
      "Epoch 17/300\n",
      "12/12 [==============================] - 23s 2s/step - loss: 37.8317 - mean_IoU: 0.5450 - val_loss: 46.2778 - val_mean_IoU: 0.5017 - lr: 0.0050\n",
      "Epoch 18/300\n",
      "12/12 [==============================] - 23s 2s/step - loss: 37.1000 - mean_IoU: 0.5584 - val_loss: 45.8084 - val_mean_IoU: 0.5116 - lr: 0.0050\n",
      "Epoch 19/300\n",
      "12/12 [==============================] - 24s 2s/step - loss: 36.3629 - mean_IoU: 0.5678 - val_loss: 39.8624 - val_mean_IoU: 0.5539 - lr: 0.0050\n",
      "Epoch 20/300\n",
      "12/12 [==============================] - 23s 2s/step - loss: 35.0438 - mean_IoU: 0.5688 - val_loss: 44.5434 - val_mean_IoU: 0.5151 - lr: 0.0050\n",
      "Epoch 21/300\n",
      "12/12 [==============================] - 23s 2s/step - loss: 32.6389 - mean_IoU: 0.5738 - val_loss: 46.6786 - val_mean_IoU: 0.5005 - lr: 0.0050\n",
      "Epoch 22/300\n",
      "12/12 [==============================] - 23s 2s/step - loss: 31.7201 - mean_IoU: 0.5769 - val_loss: 47.3607 - val_mean_IoU: 0.4993 - lr: 0.0050\n",
      "Epoch 23/300\n",
      "12/12 [==============================] - 23s 2s/step - loss: 36.4908 - mean_IoU: 0.5639 - val_loss: 47.3774 - val_mean_IoU: 0.4992 - lr: 0.0050\n",
      "Epoch 24/300\n",
      "12/12 [==============================] - 23s 2s/step - loss: 35.0879 - mean_IoU: 0.5641 - val_loss: 41.0996 - val_mean_IoU: 0.5425 - lr: 0.0050\n",
      "Epoch 25/300\n",
      "12/12 [==============================] - 23s 2s/step - loss: 34.8062 - mean_IoU: 0.5727 - val_loss: 46.3343 - val_mean_IoU: 0.5098 - lr: 0.0050\n",
      "Epoch 26/300\n",
      "12/12 [==============================] - 24s 2s/step - loss: 34.7028 - mean_IoU: 0.5576 - val_loss: 45.5227 - val_mean_IoU: 0.5153 - lr: 0.0050\n",
      "Epoch 27/300\n",
      "12/12 [==============================] - 23s 2s/step - loss: 33.7392 - mean_IoU: 0.5771 - val_loss: 46.4488 - val_mean_IoU: 0.5003 - lr: 0.0050\n",
      "Epoch 28/300\n",
      "12/12 [==============================] - 23s 2s/step - loss: 33.4333 - mean_IoU: 0.5728 - val_loss: 43.9942 - val_mean_IoU: 0.5331 - lr: 0.0050\n",
      "Epoch 29/300\n",
      "12/12 [==============================] - 23s 2s/step - loss: 32.0306 - mean_IoU: 0.5859 - val_loss: 44.2421 - val_mean_IoU: 0.5230 - lr: 0.0050\n",
      "Epoch 30/300\n",
      "12/12 [==============================] - 23s 2s/step - loss: 29.3540 - mean_IoU: 0.5946 - val_loss: 45.8154 - val_mean_IoU: 0.5115 - lr: 0.0050\n",
      "Epoch 31/300\n",
      "12/12 [==============================] - 23s 2s/step - loss: 28.7955 - mean_IoU: 0.5908 - val_loss: 44.2437 - val_mean_IoU: 0.5234 - lr: 0.0050\n",
      "Epoch 32/300\n",
      "12/12 [==============================] - 23s 2s/step - loss: 28.1007 - mean_IoU: 0.5966 - val_loss: 44.7981 - val_mean_IoU: 0.5142 - lr: 0.0050\n",
      "Epoch 33/300\n",
      "12/12 [==============================] - 23s 2s/step - loss: 29.2639 - mean_IoU: 0.5904 - val_loss: 47.7780 - val_mean_IoU: 0.4991 - lr: 0.0050\n",
      "Epoch 34/300\n",
      "12/12 [==============================] - 24s 2s/step - loss: 30.3285 - mean_IoU: 0.5927 - val_loss: 42.6015 - val_mean_IoU: 0.5500 - lr: 0.0050\n",
      "Epoch 35/300\n",
      "12/12 [==============================] - 23s 2s/step - loss: 27.1464 - mean_IoU: 0.6016 - val_loss: 41.7690 - val_mean_IoU: 0.5494 - lr: 0.0050\n",
      "Epoch 36/300\n",
      "12/12 [==============================] - 23s 2s/step - loss: 31.2273 - mean_IoU: 0.6023 - val_loss: 47.7406 - val_mean_IoU: 0.4991 - lr: 0.0050\n",
      "Epoch 37/300\n",
      "12/12 [==============================] - 23s 2s/step - loss: 29.5001 - mean_IoU: 0.5898 - val_loss: 40.9098 - val_mean_IoU: 0.5557 - lr: 0.0050\n",
      "Epoch 38/300\n",
      "12/12 [==============================] - 24s 2s/step - loss: 28.2152 - mean_IoU: 0.5979 - val_loss: 34.8308 - val_mean_IoU: 0.5959 - lr: 0.0050\n",
      "Epoch 39/300\n",
      "12/12 [==============================] - 23s 2s/step - loss: 29.3926 - mean_IoU: 0.5895 - val_loss: 26.9392 - val_mean_IoU: 0.6456 - lr: 0.0050\n",
      "Epoch 40/300\n",
      "12/12 [==============================] - 24s 2s/step - loss: 27.1918 - mean_IoU: 0.5984 - val_loss: 31.0131 - val_mean_IoU: 0.6110 - lr: 0.0050\n",
      "Epoch 41/300\n",
      "12/12 [==============================] - 23s 2s/step - loss: 27.7706 - mean_IoU: 0.6042 - val_loss: 47.6418 - val_mean_IoU: 0.4990 - lr: 0.0050\n",
      "Epoch 42/300\n",
      "12/12 [==============================] - 23s 2s/step - loss: 27.2839 - mean_IoU: 0.6085 - val_loss: 47.9443 - val_mean_IoU: 0.4990 - lr: 0.0050\n",
      "Epoch 43/300\n",
      "12/12 [==============================] - 23s 2s/step - loss: 27.8366 - mean_IoU: 0.5998 - val_loss: 43.2689 - val_mean_IoU: 0.5353 - lr: 0.0050\n",
      "Epoch 44/300\n",
      "12/12 [==============================] - 24s 2s/step - loss: 26.0220 - mean_IoU: 0.6041 - val_loss: 39.2119 - val_mean_IoU: 0.5622 - lr: 0.0050\n",
      "Epoch 45/300\n",
      "12/12 [==============================] - 24s 2s/step - loss: 26.1655 - mean_IoU: 0.5992 - val_loss: 31.5083 - val_mean_IoU: 0.6128 - lr: 0.0050\n",
      "Epoch 46/300\n",
      "12/12 [==============================] - 26s 2s/step - loss: 28.4714 - mean_IoU: 0.6006 - val_loss: 39.5059 - val_mean_IoU: 0.5648 - lr: 0.0050\n",
      "Epoch 47/300\n",
      "12/12 [==============================] - 23s 2s/step - loss: 25.4263 - mean_IoU: 0.6163 - val_loss: 24.7612 - val_mean_IoU: 0.6634 - lr: 0.0050\n",
      "Epoch 48/300\n",
      "12/12 [==============================] - 23s 2s/step - loss: 24.7831 - mean_IoU: 0.6132 - val_loss: 23.7714 - val_mean_IoU: 0.6652 - lr: 0.0050\n",
      "Epoch 49/300\n",
      "12/12 [==============================] - 23s 2s/step - loss: 26.3009 - mean_IoU: 0.6082 - val_loss: 43.1402 - val_mean_IoU: 0.5379 - lr: 0.0050\n",
      "Epoch 50/300\n",
      "12/12 [==============================] - 23s 2s/step - loss: 25.9201 - mean_IoU: 0.6102 - val_loss: 37.1140 - val_mean_IoU: 0.5876 - lr: 0.0050\n",
      "Epoch 51/300\n",
      "12/12 [==============================] - 23s 2s/step - loss: 25.8641 - mean_IoU: 0.6112 - val_loss: 25.8136 - val_mean_IoU: 0.6397 - lr: 0.0050\n",
      "Epoch 52/300\n",
      "12/12 [==============================] - 23s 2s/step - loss: 25.4420 - mean_IoU: 0.6152 - val_loss: 22.4843 - val_mean_IoU: 0.6569 - lr: 0.0050\n",
      "Epoch 53/300\n",
      "12/12 [==============================] - 23s 2s/step - loss: 26.5174 - mean_IoU: 0.6157 - val_loss: 30.0392 - val_mean_IoU: 0.6291 - lr: 0.0050\n",
      "Epoch 54/300\n",
      "12/12 [==============================] - 23s 2s/step - loss: 24.8379 - mean_IoU: 0.6084 - val_loss: 29.0446 - val_mean_IoU: 0.6267 - lr: 0.0050\n",
      "Epoch 55/300\n",
      "12/12 [==============================] - 23s 2s/step - loss: 24.8104 - mean_IoU: 0.6155 - val_loss: 26.0616 - val_mean_IoU: 0.6583 - lr: 0.0050\n",
      "Epoch 56/300\n",
      "12/12 [==============================] - 23s 2s/step - loss: 24.2118 - mean_IoU: 0.6181 - val_loss: 23.1467 - val_mean_IoU: 0.6574 - lr: 0.0050\n",
      "Epoch 57/300\n",
      "12/12 [==============================] - 23s 2s/step - loss: 24.0903 - mean_IoU: 0.6210 - val_loss: 24.7071 - val_mean_IoU: 0.6550 - lr: 0.0050\n",
      "Epoch 58/300\n",
      "12/12 [==============================] - 23s 2s/step - loss: 24.3955 - mean_IoU: 0.6150 - val_loss: 28.1847 - val_mean_IoU: 0.6399 - lr: 0.0050\n",
      "Epoch 59/300\n",
      "12/12 [==============================] - 23s 2s/step - loss: 24.2312 - mean_IoU: 0.6143 - val_loss: 22.8444 - val_mean_IoU: 0.6723 - lr: 0.0050\n",
      "Epoch 60/300\n",
      "12/12 [==============================] - 23s 2s/step - loss: 24.4433 - mean_IoU: 0.6165 - val_loss: 30.0260 - val_mean_IoU: 0.6347 - lr: 0.0050\n",
      "Epoch 61/300\n",
      "12/12 [==============================] - 24s 2s/step - loss: 23.8601 - mean_IoU: 0.6113 - val_loss: 23.0142 - val_mean_IoU: 0.6563 - lr: 0.0050\n",
      "Epoch 62/300\n",
      "12/12 [==============================] - 23s 2s/step - loss: 23.8686 - mean_IoU: 0.6130 - val_loss: 12.9673 - val_mean_IoU: 0.6999 - lr: 0.0050\n",
      "Epoch 63/300\n",
      "12/12 [==============================] - 23s 2s/step - loss: 24.8990 - mean_IoU: 0.6157 - val_loss: 25.8364 - val_mean_IoU: 0.6526 - lr: 0.0050\n",
      "Epoch 64/300\n",
      "12/12 [==============================] - 23s 2s/step - loss: 23.4765 - mean_IoU: 0.6141 - val_loss: 28.1971 - val_mean_IoU: 0.6254 - lr: 0.0050\n",
      "Epoch 65/300\n",
      "12/12 [==============================] - 24s 2s/step - loss: 24.1854 - mean_IoU: 0.6196 - val_loss: 20.6292 - val_mean_IoU: 0.6811 - lr: 0.0050\n",
      "Epoch 66/300\n",
      "12/12 [==============================] - 23s 2s/step - loss: 23.5505 - mean_IoU: 0.6273 - val_loss: 22.2539 - val_mean_IoU: 0.6794 - lr: 0.0050\n",
      "Epoch 67/300\n",
      "12/12 [==============================] - 22s 2s/step - loss: 23.6708 - mean_IoU: 0.6192 - val_loss: 24.8926 - val_mean_IoU: 0.6507 - lr: 0.0050\n",
      "Epoch 68/300\n",
      "11/12 [==========================>...] - ETA: 1s - loss: 24.1108 - mean_IoU: 0.6144"
     ]
    }
   ],
   "source": [
    "callbacks = keras.callbacks.ReduceLROnPlateau(monitor=\"loss\", factor=0.5, verbose=1, patience=10, min_lr=1e-6)\n",
    "\n",
    "smooth = [50]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "IoU = tf.keras.metrics.MeanIoU(num_classes=2, name=\"mean_IoU\")\n",
    "\n",
    "print(f\"Smoothing : {smooth}\")\n",
    "\n",
    "if smooth == 0:\n",
    "    loss = keras.losses.BinaryCrossentropy()\n",
    "else:\n",
    "    loss = jaccard_distance(smooth=smooth)\n",
    "n_filters, init_lr = 64, 0.005\n",
    "unet = get_unet(n_filters, 512, 512, n_channels_imgs=nc_ims, n_channels_masks=nc_masks)\n",
    "unet.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=init_lr),\n",
    "    # loss=keras.losses.BinaryCrossentropy(),\n",
    "    loss=loss,\n",
    "    metrics=IoU,\n",
    ")\n",
    "n_epochs = 300\n",
    "history = unet.fit(generator, validation_data=val_generator, \n",
    "                   epochs=n_epochs, verbose=1, callbacks=callbacks)\n",
    "\n",
    "ax[0].plot(history.history[\"loss\"][:], \"orange\", label=\"loss\")\n",
    "ax[0].plot(history.history[\"val_loss\"][:], \"b\", label=\"validation loss\")\n",
    "ax[0].legend()\n",
    "ax[0].set_title(\"Training curves\")\n",
    "ax[0].set_xlabel(\"Epochs\")\n",
    "ax[0].set_ylabel(\"Loss (Jaccard, smooth==50)\")\n",
    "\n",
    "ax[1].plot(history.history[\"mean_IoU\"][:], \"orange\", label=\"IoU\")\n",
    "ax[1].plot(history.history[\"val_mean_IoU\"][:], \"b\", label=\"validation IoU\")\n",
    "ax[1].legend()\n",
    "ax[1].set_title(\"Training curves\")\n",
    "ax[1].set_xlabel(\"Epochs\")\n",
    "ax[1].set_ylabel(\"IoU\")\n",
    "\n",
    "model_name = f\"BS{n_epochs}\"\n",
    "\n",
    "os.chdir(\"D:/Hugo/BiSeg/Models\")\n",
    "unet.save(model_name)\n",
    "\n",
    "plt.savefig(f\"{model_name}/{model_name}_learning_curve.png\")\n",
    "\n",
    "list_training_imgs = \"\\n\".join(os.listdir(generator.X_dir))\n",
    "list_val_imgs = \"\\n\".join(os.listdir(val_generator.X_dir))\n",
    "\n",
    "with open(f\"{model_name}/history.txt\", \"w\") as hist_file:\n",
    "    hist_file.write(f\"Model {model_name} trained for {n_epochs} epochs.\"\n",
    "                    f\"\\nNumber of training images : {len(generator)} * {generator.batch_size}, from directory {generator.X_dir}, masks from {generator.Y_dir}.\"\n",
    "                    f\"\\n\\nNumber of validation images : {len(val_generator)} * {val_generator.batch_size}, from directory {val_generator.X_dir}, masks from {val_generator.Y_dir}.\"\n",
    "                    f\"\\nLoss : {loss}, smoothing: {smooth}.\"\n",
    "                    f\"\\nNumber of filters : {n_filters}.\"\n",
    "                    f\"\\nInitial learning rate: {init_lr}.\"\n",
    "                    f\"\\n\\nList of the training images:\\n{list_training_imgs}.\"\n",
    "                    f\"\\n\\nList of the validation images:\\n{list_val_imgs}.\"\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform inference\n",
    "\n",
    "Now we will use the model to make predictions on the test dataset to check if it can generalize well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageGenerator(Sequence):\n",
    "    \"\"\"\n",
    "    Generates images and masks for performing data augmentation in Keras.\n",
    "    We inherit from Sequence (instead of directly using the keras ImageDataGenerator)\n",
    "    since we want to perform augmentation on both the input image AND the mask \n",
    "    (target). This mechanism needs to be implemented in this class. This class\n",
    "    also allows to implement new augmentation transforms that are not implemented\n",
    "    in the core Keras class (illumination, etc.).\n",
    "    See : https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "    and https://stackoverflow.com/questions/56758592/how-to-customize-imagedatagenerator-in-order-to-modify-the-target-variable-value\n",
    "    for more details.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X_set, # input images and masks\n",
    "                 n_channels_ims=1, n_channels_masks=1,\n",
    "                 batch_size: int=4, dim: tuple=(512, 512),\n",
    "                 n_channels: int=1, # informations \n",
    "                 normalize=True, reshape=False, crop=None, # preprocessing params\n",
    "                 restrict_to=\"\"): # data augmentation params\n",
    "        \"\"\"\n",
    "        X_set (list, array or str): pointer to the images (Bright-Field). If str\n",
    "        the string is assumed to be pointing at some directory.\n",
    "        Y_set (list; array or str): pointer to the masks (target). If str\n",
    "        the string is assumed to be pointing at some directory.\n",
    "        batch_size (int): size of the batch\n",
    "        dim (tuple): dimension of the images\n",
    "        n_channels (int) : number of channels of the images (1 for TIF)\n",
    "        shuffle (bool): Shuffle the dataset between each training epoch\n",
    "        normalize (bool): normalize the images and masks in the beginning\n",
    "        reshape (bool): reshape the images and masks to (dim, dim, n_channels)\n",
    "        histogram_equalization (bool): perform histogram equalization to improve\n",
    "        rendering using opencv\n",
    "        horiz_flip_percent ()\n",
    "        vert_flip_percent\n",
    "        \"\"\"\n",
    "        # super().__init__(n, batch_size, shuffle, seed)\n",
    "        self.dim = dim\n",
    "        self.im_size = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.n_channels = n_channels\n",
    "        self.n_channels_ims = n_channels_ims\n",
    "        self.n_channels_masks = n_channels_masks\n",
    "        \n",
    "        \n",
    "        self.restrict_to = restrict_to\n",
    "\n",
    "        # build the X_set in an array. If X_set is a directory containing images\n",
    "        # then self.X_set doesn't contains the images but the file names, but it\n",
    "        # is transparent for the user.\n",
    "        if type(X_set) == list:\n",
    "            self.from_directory_X = False\n",
    "            self.X_set = np.array(X_set)\n",
    "        elif type(X_set) == np.array:\n",
    "            self.from_directory_X = False\n",
    "            self.X_set = X_set           \n",
    "        elif type(X_set) == str: # assuming a path\n",
    "            self.from_directory_X = True\n",
    "            self.X_dir = X_set # path to the images dir\n",
    "            self.X_set = []\n",
    "            if self.restrict_to == \"\":\n",
    "                for k in range(0, len(os.listdir(X_set)), self.n_channels_ims):\n",
    "                    self.X_set.append(np.array(os.listdir(X_set)[k:k+self.n_channels_ims]))\n",
    "                self.X_set = np.array(self.X_set)\n",
    "            else:\n",
    "                for k in range(0, len(os.listdir(X_set)), self.n_channels_ims):\n",
    "                    if os.listdir(X_set)[k].startswith(self.restrict_to):\n",
    "                        self.X_set.append(np.array(os.listdir(X_set)[k:k+self.n_channels_ims]))\n",
    "                self.X_set = np.array(self.X_set)\n",
    "        else:\n",
    "            raise TypeError(\"X_set should be list, array or path\")\n",
    "        \n",
    "        # Preprocessing parameters\n",
    "        self.normalize = normalize\n",
    "        self.reshape = reshape\n",
    "        self.crop = crop\n",
    "        \n",
    "        # Initialize the indices (shuffle if asked)\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Number of batches per epoch : we evenly split the train set into samples\n",
    "        of size batch_size.\n",
    "        \"\"\"\n",
    "        return int(np.floor(self.X_set.shape[0] / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        \"\"\"\n",
    "        Generate one batch of data.\n",
    "        \"\"\"\n",
    "        if index >= self.__len__():\n",
    "            raise IndexError\n",
    "            \n",
    "        # Generate indices corresponding to the images in the batch\n",
    "        indices = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        # Generate the batch\n",
    "        X = self.__data_generation(indices)\n",
    "        return X\n",
    "    \n",
    "    def get_image_idx(self, im_name):\n",
    "        \"\"\"\n",
    "        Used to sort the images by an idx, when they are properly sorted (e.g. when images\n",
    "        are numbered 1 to 1000 instead of 0001 to 1000). We assume that the numerical index\n",
    "        is in the form \"XXX_tnumericalindex.tiff\" where XXC can be anything.\n",
    "        \"\"\"\n",
    "        if \"-\" in im_name.split(\".\")[0].split(\"_\")[-1][1:]:\n",
    "            return int(im_name.split(\".\")[0].split(\"_\")[-1][1:].split(\"-\")[-1][1:])\n",
    "        else:\n",
    "            return int(im_name.split(\".\")[0].split(\"_\")[-1][1:])\n",
    "        \n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"\n",
    "        Updates indexes after each epoch. self.indexes is used to retrieve the\n",
    "        samples and organize them into batches.\n",
    "        If shuffle : randomizes the order of the samples in order to give \n",
    "        different training batches at each epoch.\n",
    "        \"\"\"\n",
    "        self.indexes = np.arange(self.X_set.shape[0])\n",
    "\n",
    "    def __data_generation(self, list_IDs: [int]):\n",
    "        \"\"\"\n",
    "        Generates data containing batch_size samples. This is where we load the\n",
    "        images if they are in a directory, and apply transformations to them.\n",
    "        \"\"\" \n",
    "        # Load data (from directory or from X_set depending on the given data)\n",
    "        if self.from_directory_X:\n",
    "            batch_X = []\n",
    "            for im in list_IDs:\n",
    "                channels = []\n",
    "                for k in range(self.n_channels_ims):\n",
    "                    channels.append(np.expand_dims(imageio.imread(f\"{self.X_dir}/{self.X_set[im, k]}\"), axis=-1)) # add channel axis\n",
    "                batch_X.append(np.concatenate(channels, axis=-1))\n",
    "            batch_X = np.array(batch_X)            \n",
    "        else:\n",
    "            batch_X = self.X_set[list_IDs]\n",
    "\n",
    "        # Preprocessing\n",
    "        if self.crop is not None:\n",
    "            batch_X = self.perf_crop(batch_X)\n",
    "            \n",
    "        if self.reshape:\n",
    "            batch_X = self.perf_reshape(batch_X)\n",
    "\n",
    "        if self.normalize:\n",
    "            batch_X = self.perf_normalize(batch_X)\n",
    "\n",
    "        return batch_X\n",
    "\n",
    "    # Preprocessing functions\n",
    "    def perf_crop(self, images):\n",
    "        crop_X = int((images.shape[1] - self.crop[0]) // 2)\n",
    "        crop_Y = int((images.shape[2] - self.crop[1]) // 2)\n",
    "        new_batch = np.empty((self.batch_size, *self.crop))\n",
    "        for i, img in enumerate(images):\n",
    "            if crop_X != 0 and crop_Y != 0:\n",
    "                new_batch[i] = img[crop_X:-crop_X, crop_Y:-crop_Y]\n",
    "            elif crop_X != 0:\n",
    "                new_batch[i] = img[crop_X:-crop_X, :]\n",
    "            elif crop_Y != 0:\n",
    "                new_batch[i] = img[:, crop_Y:-crop_Y]\n",
    "            else:\n",
    "                new_batch[i] = img\n",
    "        return new_batch\n",
    "    \n",
    "    def perf_reshape(self, images):\n",
    "        \"\"\"\n",
    "        images (np.array): batch of images of shape (batch_size, n_rows, n_cols, n_chans)\n",
    "        is_images (bool): is it a batch of images (True) or masks (False)\n",
    "        \"\"\"\n",
    "        new_batch = np.empty((self.batch_size, *self.im_size, self.n_channels_ims))\n",
    "        for i, img in enumerate(images): # the resize function normalizes the images anyways...\n",
    "            new_batch[i] = skimage.transform.resize(img, (*self.im_size, self.n_channels_ims), anti_aliasing=True)\n",
    "        return new_batch\n",
    "\n",
    "    def perf_normalize(self, images):\n",
    "        \"\"\"\n",
    "        Performs per image, per channel normalization by substracting the min and dividing by (max - min)\n",
    "        \"\"\"\n",
    "        new_batch = np.empty(images.shape)\n",
    "        for i, img in enumerate(images):\n",
    "            assert (np.min(img, axis=(0, 1)) != np.max(img, axis=(0, 1))).all(), print(\"Cannot normalize an image containing only 0 or 1 valued pixels. There is likely an empty image in the training set.\\nIf cropping was used,\"\n",
    "                                                                                       \"maybe the mask doesn't contain any white pixel in the specific region.\")\n",
    "            new_batch[i] = (img - np.min(img, axis=(0, 1))) / (np.max(img, axis=(0, 1)) - np.min(img, axis=(0, 1)))\n",
    "        return new_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE DATASET PATH HERE\n",
    "test_path = \"D:\\Hugo\\BiSeg\\Test_Set/H449_1-120\"\n",
    "restrict_to = \"\"\n",
    "bs, n_chan_ims, n_chan_ms = 1, 2, 1\n",
    "test_set = ImageGenerator(test_path, batch_size=bs, dim=(512, 512),\n",
    "                          n_channels_ims=n_chan_ims, n_channels_masks=n_chan_ms, crop=None, normalize=True, reshape=True, restrict_to=restrict_to)\n",
    "\n",
    "visualize_data(test_set[0], np.zeros((1, test_set[0].shape[0], test_set[0].shape[1], 1)), n_chan_ims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 512, 512, 2)\n"
     ]
    }
   ],
   "source": [
    "predictions = unet.predict(test_set)\n",
    "\n",
    "whole_test_set = np.concatenate([test_set[i] for i in range(len(test_set))], axis=0)\n",
    "print(whole_test_set.shape)\n",
    "\n",
    "def visualize_data_and_predictions(bf, preds, nc_ims=1):\n",
    "    with napari.gui_qt():\n",
    "        if nc_ims == 1:\n",
    "            viewer = napari.view_image(bf[:, :, :, :].squeeze(-1))\n",
    "            viewer.add_image(preds[:, :, :, :].squeeze(-1), blending=\"additive\", name=\"BF\")\n",
    "        else:\n",
    "            viewer = napari.view_image(bf[:, :, :, 1], name=\"BF\")  # bf\n",
    "            viewer.add_image(bf[:, :, :, 0], blending=\"additive\", name=\"Fluo\", colormap=\"red\")\n",
    "            viewer.add_image(preds[:, :, :, :].squeeze(-1), blending=\"additive\", name=\"Predictions\", colormap=\"blue\")\n",
    "\n",
    "plot = True\n",
    "if plot:\n",
    "    visualize_data_and_predictions(whole_test_set, predictions, nc_ims=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_predictions = True\n",
    "if save_predictions:\n",
    "    # REPLACE TEST SAVE PREDICTIONS PATH\n",
    "    save_path = \"D:/Hugo/BiSeg/Predictions/BS200-H449_pos2_1-120.tif\"\n",
    "    predicted_nochan = predictions.squeeze(-1)\n",
    "\n",
    "    imageio.volwrite(save_path, predicted_nochan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: S1\\assets\n"
     ]
    }
   ],
   "source": [
    "# CHANGE SAVE PATH HERE\n",
    "os.chdir(\"D:/Hugo/Whole_Cell/Models/\")\n",
    "unet.save(\"S1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REPLACE SAVE MODEL PATH\n",
    "save_model_path = \"/content/gdrive/MyDrive/CYBERSCOPE/Migration/Models\"\n",
    "unet.save(save_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Train_from_scratch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
